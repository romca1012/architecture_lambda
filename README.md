üåÄ Lambda Log Processing ‚Äì Projet d'Architecture Lambda (Kafka + Spark)
üìö Contexte
Ce projet met en ≈ìuvre une architecture Lambda pour analyser en temps r√©el et en batch des logs de connexions utilisateurs. Il combine Kafka, Spark Streaming, Spark Batch, et expose les r√©sultats via une API FastAPI.

‚öôÔ∏è Technologies utilis√©es
Kafka (Streaming des logs en temps r√©el)

Apache Spark 3.5.1

Streaming (consommation Kafka)

Batch (traitement et agr√©gation)

FastAPI (exposition des m√©triques)

Docker & Docker Compose

Python 3.12, Pandas, Uvicorn

Parquet pour stockage structur√©

üèóÔ∏è Architecture
lua
Copier
Modifier
+------------------+        +-------------------------+        +--------------------+
|  log_generator.py|  ==>   |   Kafka (topic logs)    |  ==>   | Spark Streaming    |
|  (fichier Python)|        |                         |        | (spark_streaming_) |
+------------------+        +-------------------------+        +--------------------+
                                                                      |
                                                                      v
                                             +-------------------------------+
                                             | Parquet (data/logs_streaming) |
                                             +-------------------------------+

                                                      ‚áì

                                            +----------------------+
                                            | Spark Batch Job      |
                                            | (traitement des logs)|
                                            +----------------------+
                                                      ‚áì
                                             data/batch_results/
                                                      ‚áì
                                             API FastAPI (3 endpoints)
üöÄ Lancer le projet
1. D√©marrage de l'environnement Kafka
docker-compose up -d
2. G√©n√©ration des logs (envoie dans Kafka)
python log_generator.py
3. Lancement de Spark Streaming
./spark-3.5.1-bin-hadoop3/bin/spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
  spark_streaming_consumer.py
4. Lancement du traitement batch
spark-shell
scala
Copier
Modifier
// Exemple :
val df = spark.read.parquet("data/logs_streaming")
val result = df.groupBy("ip").count().withColumnRenamed("count", "nb_connexions")
result.write.option("header", true).mode("overwrite").csv("data/batch_results/connexions_par_ip")
5. Lancer l'API FastAPI
uvicorn api.main:app --reload
‚û°Ô∏è Acc√©der √† la documentation : http://127.0.0.1:8000/docs
